{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building and Visualizing word frequencies\n",
    "\n",
    "\n",
    "We will focus on the `build_freqs()` helper function and visualizing a dataset fed into it. In our goal of tweet sentiment analysis, this function will build a dictionary where we can lookup how many times a word appears in the lists of positive or negative tweets. This will be very helpful when extracting the features of the dataset. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                  # Python library for NLP\n",
    "from nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt              # visualization library\n",
    "import numpy as np                           # library for scientific computing and matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import some helper functions that we provided in the utils.py file:\n",
    "* `process_tweet()`: Cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n",
    "* `build_freqs()`: This counts how often a word in the 'corpus' (the entire set of tweets) was associated with a positive label `1` or a negative label `0`. It then builds the `freqs` dictionary, where each key is a `(word,label)` tuple, and the value is the count of its frequency within the corpus of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the stopwords for the process_tweet function\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# import our convenience functions\n",
    "from nlp.utils import process_tweet, build_freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Dataset\n",
    "\n",
    "# select the lists of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# concatenate the lists, 1st part is the positive tweets followed by the negative\n",
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "\n",
    "# let's see how many tweets we have\n",
    "print(\"Number of tweets: \", len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array representing labels of the tweets\n",
    "labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries\n",
    "\n",
    "In NLP, dictionaries are essential because it enables fast retrieval of items or containment checks even with thousands of entries in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accessing values and lookup keys\n",
    "\n",
    "Performing dictionary lookups and retrieval are common tasks in NLP. There are two ways to do this: \n",
    "\n",
    "* Using square bracket notation: This form is allowed if the lookup key is in the dictionary. It produces an error otherwise.\n",
    "* Using the get()-method: This allows us to set a default value if the dictionary key does not exist. \n",
    "\n",
    "Let us see these in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'key1': 1, 'key2': 2}\n",
    "print(d['key2'])\n",
    "print(\"item found: \", d.get('key1', -1)) # set the default to -1 when not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(tweets, labels)\n",
    "\n",
    "# check data type\n",
    "print(f'type(freqs) = {type(freqs)}')\n",
    "\n",
    "# check length of the dictionary\n",
    "print(f'len(freqs) = {len(freqs)}')\n",
    "\n",
    "# print the frequency of each word depending on its class.\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of word counts\n",
    "\n",
    "We will select a set of words that we would like to visualize. It is better to store this temporary information in a table that is very easy to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\n",
    "keys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n",
    "        '‚ù§', ':)', ':(', 'üòí', 'üò¨', 'üòÑ', 'üòç', '‚ôõ',\n",
    "        'song', 'idea', 'power', 'play', 'magnific']\n",
    "\n",
    "# list representing our table of word counts.\n",
    "# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\n",
    "data = []\n",
    "\n",
    "# loop through our selected words\n",
    "for word in keys:\n",
    "    \n",
    "    # initialize positive and negative counts\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    # retrieve number of positive counts\n",
    "    if (word, 1) in freqs:\n",
    "        pos = freqs[(word, 1)]\n",
    "        \n",
    "    # retrieve number of negative counts\n",
    "    if (word, 0) in freqs:\n",
    "        neg = freqs[(word, 0)]\n",
    "        \n",
    "    # append the word counts to the table\n",
    "    data.append([word, pos, neg])\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use a scatter plot to inspect this table visually. Instead of plotting the raw counts, we will plot it in the logarithmic scale to take into account the wide discrepancies between the raw counts (e.g. `:)` has 3568 counts in the positive while only 2 in the negative). The red line marks the boundary between positive and negative areas. Words close to the red line can be classified as neutral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\n",
    "x = np.log([x[1] + 1 for x in data])  \n",
    "\n",
    "# do the same for the negative counts\n",
    "y = np.log([x[2] + 1 for x in data]) \n",
    "\n",
    "# Plot a dot for each pair of words\n",
    "ax.scatter(x, y)  \n",
    "\n",
    "# assign axis labels\n",
    "plt.xlabel(\"Log Positive count\")\n",
    "plt.ylabel(\"Log Negative count\")\n",
    "\n",
    "# Add the word as the label at the same position as you added the points just before\n",
    "for i in range(0, len(data)):\n",
    "    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n",
    "\n",
    "ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
